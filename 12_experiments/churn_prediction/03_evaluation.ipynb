{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Churn Prediction: Evaluation & Business Impact\n",
                "\n",
                "In this final notebook, we evaluate models comprehensively and translate technical metrics into business value.\n",
                "\n",
                "## Goals\n",
                "1. Deep dive into confusion matrix and error analysis\n",
                "2. Cost-benefit analysis of different thresholds\n",
                "3. Feature importance interpretation\n",
                "4. Real-world deployment considerations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import StandardScaler,OneHotEncoder\n",
                "from sklearn.compose import ColumnTransformer\n",
                "from sklearn.pipeline import Pipeline\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "from sklearn.metrics import (\n",
                "    confusion_matrix, classification_report, roc_auc_score,\n",
                "    roc_curve, precision_recall_curve, f1_score\n",
                ")\n",
                "\n",
                "# Load and prep data (same as before)\n",
                "df = pd.read_csv('data/telco_churn.csv')\n",
                "df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce').fillna(0)\n",
                "df['Churn'] = (df['Churn'] == 'Yes').astype(int)\n",
                "df.drop('customerID', axis=1, inplace=True)\n",
                "\n",
                "X = df.drop('Churn', axis=1)\n",
                "y = df['Churn']\n",
                "\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
                "\n",
                "# Rebuild model (best from 02_modeling.ipynb)\n",
                "num_cols = ['tenure', 'MonthlyCharges', 'TotalCharges']\n",
                "cat_cols = X.select_dtypes(include=['object']).columns\n",
                "\n",
                "preprocessor = ColumnTransformer([\n",
                "    ('num', StandardScaler(), num_cols),\n",
                "    ('cat', OneHotEncoder(drop='first', handle_unknown='ignore'), cat_cols)\n",
                "])\n",
                "\n",
                "model = Pipeline([\n",
                "    ('preprocessor', preprocessor),\n",
                "    ('classifier', RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42))\n",
                "])\n",
                "\n",
                "model.fit(X_train, y_train)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Confusion Matrix Analysis\n",
                "\n",
                "Understanding **where** the model fails is as important as the overall accuracy."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "y_pred = model.predict(X_test)\n",
                "y_prob = model.predict_proba(X_test)[:, 1]\n",
                "\n",
                "cm = confusion_matrix(y_test, y_pred)\n",
                "\n",
                "plt.figure(figsize=(8, 6))\n",
                "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
                "            xticklabels=['Stay', 'Churn'], yticklabels=['Stay', 'Churn'])\n",
                "plt.ylabel('Actual')\n",
                "plt.xlabel('Predicted')\n",
                "plt.title('Confusion Matrix')\n",
                "plt.show()\n",
                "\n",
                "tn, fp, fn, tp = cm.ravel()\n",
                "print(f\"True Negatives: {tn}\")\n",
                "print(f\"False Positives: {fp} (wasted retention offers)\")\n",
                "print(f\"False Negatives: {fn} (missed churners - CRITICAL)\")\n",
                "print(f\"True Positives: {tp} (saved customers)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "> **DECISION CHECKPOINT 1**: Interpreting Errors\n",
                ">\n",
                "> - **False Negatives (FN)**: These are churners we missed. They leave without intervention.\n",
                "> - **False Positives (FP)**: We sent retention offers to people who would have stayed anyway.\n",
                ">\n",
                "> **Business Question**: Which error is more expensive?\n",
                "> - FN cost: Lost customer = Lost LTV ($2000)\n",
                "> - FP cost: Wasted offer = $100\n",
                ">\n",
                "> **Conclusion**: FN is 20x more expensive. We MUST minimize false negatives, even at the cost of more false positives."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. ROC Curve and AUC"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
                "roc_auc = roc_auc_score(y_test, y_prob)\n",
                "\n",
                "plt.figure(figsize=(8, 6))\n",
                "plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.3f})')\n",
                "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
                "plt.xlabel('False Positive Rate')\n",
                "plt.ylabel('True Positive Rate (Recall)')\n",
                "plt.title('ROC Curve')\n",
                "plt.legend()\n",
                "plt.grid()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Precision-Recall Tradeoff & Custom Threshold"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "precision, recall, pr_thresholds = precision_recall_curve(y_test, y_prob)\n",
                "\n",
                "# Calculate F1 for each threshold\n",
                "f1_scores = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
                "\n",
                "plt.figure(figsize=(14, 5))\n",
                "\n",
                "# Plot 1: Precision-Recall curve\n",
                "plt.subplot(1, 2, 1)\n",
                "plt.plot(recall, precision, label='PR Curve')\n",
                "plt.xlabel('Recall')\n",
                "plt.ylabel('Precision')\n",
                "plt.title('Precision-Recall Curve')\n",
                "plt.legend()\n",
                "plt.grid()\n",
                "\n",
                "# Plot 2: Metrics vs Threshold\n",
                "plt.subplot(1, 2, 2)\n",
                "plt.plot(pr_thresholds, precision[:-1], label='Precision')\n",
                "plt.plot(pr_thresholds, recall[:-1], label='Recall')\n",
                "plt.plot(pr_thresholds, f1_scores[:-1], label='F1', linestyle='--')\n",
                "plt.xlabel('Threshold')\n",
                "plt.ylabel('Score')\n",
                "plt.title('Metrics vs Threshold')\n",
                "plt.axvline(x=0.5, color='red', linestyle='--', alpha=0.5, label='Default (0.5)')\n",
                "plt.axvline(x=0.3, color='green', linestyle='--', alpha=0.5, label='Custom (0.3)')\n",
                "plt.legend()\n",
                "plt.grid()\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "> **DECISION CHECKPOINT 2**: Threshold Selection\n",
                ">\n",
                "> Given our cost analysis:\n",
                "> - Lowering threshold from 0.5 â†’ 0.3 increases Recall significantly\n",
                "> - Precision drops slightly, but we can afford more false positives\n",
                ">\n",
                "> **Action**: Deploy with threshold = 0.3"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Cost-Benefit Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Business parameters\n",
                "LTV = 2000  # Customer lifetime value\n",
                "OFFER_COST = 100\n",
                "ACCEPTANCE_RATE = 0.5  # 50% of churners accept retention offer\n",
                "\n",
                "def calculate_roi(threshold):\n",
                "    y_pred_custom = (y_prob >= threshold).astype(int)\n",
                "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred_custom).ravel()\n",
                "    \n",
                "    # Revenue from saved customers\n",
                "    saved_customers = tp * ACCEPTANCE_RATE\n",
                "    revenue = saved_customers * LTV\n",
                "    \n",
                "    # Cost of offers\n",
                "    total_offers = tp + fp\n",
                "    cost = total_offers * OFFER_COST\n",
                "    \n",
                "    # Lost revenue from missed churners\n",
                "    lost_revenue = fn * LTV * ACCEPTANCE_RATE\n",
                "    \n",
                "    roi = revenue - cost - lost_revenue\n",
                "    \n",
                "    return {\n",
                "        'threshold': threshold,\n",
                "        'tp': tp, 'fp': fp, 'fn': fn, 'tn': tn,\n",
                "        'revenue': revenue,\n",
                "        'cost': cost,\n",
                "        'lost_revenue': lost_revenue,\n",
                "        'net_roi': roi\n",
                "    }\n",
                "\n",
                "# Compare different thresholds\n",
                "thresholds_to_test = [0.2, 0.3, 0.4, 0.5, 0.6]\n",
                "results = [calculate_roi(t) for t in thresholds_to_test]\n",
                "\n",
                "df_roi = pd.DataFrame(results)\n",
                "print(df_roi[['threshold', 'tp', 'fp', 'fn', 'net_roi']])\n",
                "\n",
                "# Plot ROI vs Threshold\n",
                "plt.figure(figsize=(10, 6))\n",
                "plt.plot(df_roi['threshold'], df_roi['net_roi'], marker='o', linewidth=2)\n",
                "plt.xlabel('Prediction Threshold')\n",
                "plt.ylabel('Net ROI ($)')\n",
                "plt.title('Business ROI vs Prediction Threshold')\n",
                "plt.grid()\n",
                "optimal_idx = df_roi['net_roi'].idxmax()\n",
                "optimal_threshold = df_roi.loc[optimal_idx, 'threshold']\n",
                "plt.axvline(x=optimal_threshold, color='green', linestyle='--', \n",
                "            label=f'Optimal: {optimal_threshold}')\n",
                "plt.legend()\n",
                "plt.show()\n",
                "\n",
                "print(f\"\\nOptimal threshold for ROI: {optimal_threshold}\")\n",
                "print(f\"Expected annual ROI: ${df_roi.loc[optimal_idx, 'net_roi']:,.0f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Error Analysis: Which Customers Are We Missing?\n",
                "\n",
                "Let's examine false negatives to understand model blind spots."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get false negatives\n",
                "X_test_df = X_test.copy()\n",
                "X_test_df['actual'] = y_test.values\n",
                "X_test_df['predicted'] = y_pred\n",
                "X_test_df['probability'] = y_prob\n",
                "\n",
                "false_negatives = X_test_df[(X_test_df['actual'] == 1) & (X_test_df['predicted'] == 0)]\n",
                "true_positives = X_test_df[(X_test_df['actual'] == 1) & (X_test_df['predicted'] == 1)]\n",
                "\n",
                "print(f\"False Negatives: {len(false_negatives)}\")\n",
                "print(f\"\\nAverage probability for FN: {false_negatives['probability'].mean():.3f}\")\n",
                "print(f\"Average probability for TP: {true_positives['probability'].mean():.3f}\")\n",
                "\n",
                "# Compare feature distributions\n",
                "print(\"\\nFeature comparison (FN vs TP):\")\n",
                "for col in ['tenure', 'MonthlyCharges']:\n",
                "    if col in false_negatives.columns:\n",
                "        print(f\"{col}: FN={false_negatives[col].mean():.1f}, TP={true_positives[col].mean():.1f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "> **DECISION CHECKPOINT 3**: Model Limitations\n",
                ">\n",
                "> False negatives tend to have:\n",
                "> - Higher tenure (long-term customers who suddenly churn)\n",
                "> - Lower monthly charges\n",
                ">\n",
                "> **Insight**: The model struggles with \"surprise\" churners who don't fit the typical pattern.\n",
                ">\n",
                "> **Action for Future**: Add features like:\n",
                "> - Recent service changes\n",
                "> - Support ticket frequency\n",
                "> - Payment delays"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Real-World Deployment Considerations\n",
                "\n",
                "### A. Data Drift Monitoring\n",
                "\n",
                "After deployment, we must monitor whether data distribution changes:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Simulate monitoring (compare train vs test distributions)\n",
                "print(\"Feature Drift Check:\")\n",
                "print(f\"Train tenure mean: {X_train['tenure'].mean():.1f}\")\n",
                "print(f\"Test tenure mean: {X_test['tenure'].mean():.1f}\")\n",
                "print(f\"\\nTrain churn rate: {y_train.mean():.1%}\")\n",
                "print(f\"Test churn rate: {y_test.mean():.1%}\")\n",
                "\n",
                "# If test churn rate >> train churn rate, retrain!"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### B. Metric Misuse Example: The Accuracy Trap\n",
                "\n",
                "Let's show why accuracy is misleading for this problem."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Dummy baseline: Always predict \"No Churn\"\n",
                "baseline_pred = np.zeros_like(y_test)\n",
                "baseline_accuracy = (baseline_pred == y_test).mean()\n",
                "\n",
                "model_accuracy = (y_pred == y_test).mean()\n",
                "\n",
                "print(f\"Baseline (always 'No Churn') Accuracy: {baseline_accuracy:.1%}\")\n",
                "print(f\"Our Model Accuracy: {model_accuracy:.1%}\")\n",
                "print(f\"\\nBut baseline catches 0% of churners!\")\n",
                "print(f\"Our model catches {recall:.1%} of churners (Recall)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "> **METRIC MISUSE EXAMPLE**: \n",
                ">\n",
                "> If we optimized for accuracy, we'd just predict \"No Churn\" for everyone and get 73% accuracy!\n",
                "> But we'd miss 100% of churners.\n",
                ">\n",
                "> **Lesson**: For imbalanced problems, optimize for the metric that matters to business (Recall in this case)."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Summary\n",
                "\n",
                "### Key Findings\n",
                "1. **Optimal threshold**: 0.3 (not default 0.5)\n",
                "2. **Expected ROI**: Calculated based on business costs\n",
                "3. **Model blind spot**: Long-tenure surprise churners\n",
                "4. **Critical metric**: Recall (catching churners) over Precision\n",
                "\n",
                "### Deployment Checklist\n",
                "- [ ] Set prediction threshold to 0.3\n",
                "- [ ] Monitor churn rate weekly for drift\n",
                "- [ ] Track false negative characteristics\n",
                "- [ ] Retrain quarterly with fresh data\n",
                "- [ ] A/B test retention offers to measure true acceptance rate"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}