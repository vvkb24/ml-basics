# ðŸš€ Advanced Learning Path

Deep learning, transformers, and cutting-edge techniques.

## Prerequisites

Complete the [Intermediate Path](./intermediate.md) or have equivalent knowledge of:
- Classical ML algorithms
- Optimization theory
- Basic probability and statistics

---

## Module 1: Neural Network Foundations

### Week 1-2: Perceptron to MLP
- Single-layer perceptron
- Multi-layer architectures
- Activation functions
- Universal approximation theorem

**Resources:**
- [Perceptron](../../05_neural_networks/perceptron/)
- [Multi-layer NN](../../05_neural_networks/multilayer_nn/)

### Week 3-4: Backpropagation Deep Dive
- Computational graphs
- Chain rule in matrix form
- Gradient flow
- Vanishing/exploding gradients

**Resources:**
- [Backpropagation](../../05_neural_networks/backpropagation/)

### Week 5: Optimization Methods
- SGD and momentum
- AdaGrad, RMSprop, Adam
- Learning rate schedules
- Batch normalization

**Resources:**
- [Optimization](../../05_neural_networks/optimization_methods/)

---

## Module 2: Convolutional Neural Networks

### Week 6-7: CNN Architecture
- Convolution operation
- Pooling layers
- Classic architectures (LeNet, AlexNet, VGG)
- Modern architectures (ResNet, Inception)

**Resources:**
- [CNN](../../06_deep_learning/cnn/)

### Week 8: CNN Applications
- Image classification
- Object detection
- Transfer learning

---

## Module 3: Sequence Models

### Week 9-10: RNNs and LSTMs
- Recurrent neural networks
- Long-term dependencies
- LSTM and GRU cells
- Bidirectional RNNs

**Resources:**
- [RNN](../../06_deep_learning/rnn/)
- [LSTM/GRU](../../06_deep_learning/lstm_gru/)

---

## Module 4: Attention and Transformers

### Week 11-12: Attention Mechanisms
- Seq2seq with attention
- Self-attention
- Multi-head attention

**Resources:**
- [Attention](../../06_deep_learning/attention/)
- [Self-Attention](../../07_transformers/self_attention/)

### Week 13-14: Transformers
- Positional encoding
- Transformer architecture
- BERT and GPT fundamentals
- Fine-tuning strategies

**Resources:**
- [Transformer from Scratch](../../07_transformers/transformer_from_scratch/)
- [Positional Encoding](../../07_transformers/positional_encoding/)
- [LLM Fundamentals](../../07_transformers/llm_fundamentals/)

---

## Module 5: LLM Applications

### Week 15-16
- Text generation
- Prompt engineering
- Retrieval-Augmented Generation (RAG)
- Fine-tuning LLMs

**Resources:**
- [Text Generation](../../09_applications/llm_apps/text_generation.ipynb)
- [RAG Pipeline](../../09_applications/llm_apps/rag_pipeline.md)

---

## Projects

1. **Image Classification**: Build a CNN classifier from scratch
2. **Sentiment Analysis**: LSTM-based text classification
3. **Transformer Implementation**: Attention is all you need
4. **LLM Application**: Build a RAG system

---

## Research Directions

After completing this path, explore:
- Generative models (VAE, GAN, Diffusion)
- Reinforcement learning
- Graph neural networks
- Multimodal learning
