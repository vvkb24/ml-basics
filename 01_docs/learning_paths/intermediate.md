# ðŸ“ˆ Intermediate Learning Path

Build on your foundations with core ML algorithms and techniques.

## Prerequisites

Complete the [Beginner Path](./beginner.md) or have equivalent knowledge of:
- NumPy and Matplotlib
- Linear algebra basics
- Linear regression

---

## Module 1: Classification

### Week 1-2: Logistic Regression
- Binary classification problem
- Sigmoid function and log-odds
- Maximum likelihood estimation
- Gradient descent optimization

**Resources:**
- [Theory](../../algorithms/supervised/logistic_regression/theory.md)
- [Implementation](../../algorithms/supervised/logistic_regression/scratch.py)

### Week 3: K-Nearest Neighbors
- Distance metrics (Euclidean, Manhattan, Minkowski)
- Choosing K
- Curse of dimensionality

**Resources:**
- [Theory](../../algorithms/supervised/knn/theory.md)
- [Implementation](../../algorithms/supervised/knn/scratch.py)

---

## Module 2: Advanced Classification

### Week 4-5: Support Vector Machines
- Maximum margin classifiers
- Kernel trick
- Soft margin and regularization
- Dual formulation

**Resources:**
- [Theory](../../algorithms/supervised/svm/theory.md)
- [Implementation](../../algorithms/supervised/svm/scratch.py)

### Week 6-7: Decision Trees
- Information gain and entropy
- Gini impurity
- Tree pruning
- Handling continuous features

**Resources:**
- [Theory](../../algorithms/supervised/decision_trees/theory.md)
- [Implementation](../../algorithms/supervised/decision_trees/scratch.py)

---

## Module 3: Unsupervised Learning

### Week 8-9: Clustering
- K-Means algorithm
- Hierarchical clustering
- Silhouette score
- Elbow method

**Resources:**
- [K-Means](../../algorithms/unsupervised/kmeans/)
- [Hierarchical](../../algorithms/unsupervised/hierarchical_clustering/)

### Week 10: Gaussian Mixture Models
- Probability density estimation
- Expectation-Maximization algorithm
- Model selection with BIC/AIC

**Resources:**
- [GMM/EM](../../algorithms/unsupervised/gmm_em/)

---

## Module 4: Ensemble Methods

### Week 11-12: Ensemble Learning
- Bagging and Random Forests
- Boosting (AdaBoost, Gradient Boosting)
- Feature importance

**Resources:**
- [Random Forest](../../algorithms/ensemble_methods/random_forest/)
- [Gradient Boosting](../../algorithms/ensemble_methods/gradient_boosting/)

---

## Module 5: Dimensionality Reduction

### Week 13-14
- Principal Component Analysis (PCA)
- Linear Discriminant Analysis (LDA)
- t-SNE and UMAP for visualization

**Resources:**
- [PCA](../../dimensionality_reduction/pca/)
- [t-SNE/UMAP](../../dimensionality_reduction/tsne_umap/)

---

## Projects

Apply your knowledge with these projects:
1. **Binary Classification**: Spam detection
2. **Multi-class Classification**: Handwritten digit recognition
3. **Clustering**: Customer segmentation

---

## Next Steps

Ready for deep learning? Continue to the [Advanced Path](./advanced.md).
