# Machine Learning Glossary

Quick reference for common ML terms.

---

## A

**Accuracy**: Fraction of correct predictions.

**Activation Function**: Non-linear function applied to neuron outputs (ReLU, sigmoid, tanh).

**Adam**: Adaptive moment estimation optimizer combining momentum and RMSprop.

**Attention**: Mechanism allowing models to focus on relevant parts of input.

**AUC**: Area Under the ROC Curve; measures classifier performance.

---

## B

**Backpropagation**: Algorithm for computing gradients via chain rule.

**Batch Normalization**: Normalizing layer inputs to stabilize training.

**Batch Size**: Number of samples processed before parameter update.

**Bias (model)**: Systematic error from simplifying assumptions.

**Bias (parameter)**: Constant term in linear models.

---

## C

**Classification**: Predicting discrete class labels.

**CNN**: Convolutional Neural Network; uses convolution operations for spatial data.

**Convex Function**: Function where any local minimum is global minimum.

**Cross-Entropy**: Loss function for classification; measures divergence between distributions.

**Cross-Validation**: Technique for estimating generalization by training on data subsets.

---

## D

**Decision Boundary**: Surface separating classes in feature space.

**Dimensionality Reduction**: Reducing number of features while preserving information.

**Dropout**: Regularization technique randomly zeroing activations during training.

---

## E

**Embedding**: Dense vector representation of discrete data.

**Ensemble**: Combining multiple models for better predictions.

**Epoch**: One complete pass through training data.

**Entropy**: Measure of uncertainty in a distribution.

---

## F

**F1 Score**: Harmonic mean of precision and recall.

**Feature**: Input variable used for prediction.

**Feature Engineering**: Creating new features from raw data.

**Fine-tuning**: Adapting pre-trained model to new task.

---

## G

**Generalization**: Model's ability to perform on unseen data.

**Gradient**: Vector of partial derivatives indicating direction of steepest ascent.

**Gradient Descent**: Optimization algorithm following negative gradient.

**GPU**: Graphics Processing Unit; accelerates parallel computations.

---

## H

**Hyperparameter**: Parameter set before training (learning rate, regularization).

**Hypothesis Space**: Set of all possible models under consideration.

---

## I

**Inference**: Making predictions with trained model.

**Information Gain**: Reduction in entropy from splitting data.

---

## K

**Kernel**: Function computing similarity between data points (SVM).

**K-Fold Cross-Validation**: Dividing data into K parts for validation.

**KL Divergence**: Measure of difference between probability distributions.

---

## L

**L1 Regularization**: Adds sum of absolute weights to loss (Lasso).

**L2 Regularization**: Adds sum of squared weights to loss (Ridge).

**Learning Rate**: Step size for gradient descent.

**Logistic Regression**: Linear model for classification using sigmoid.

**Loss Function**: Measures model error; objective to minimize.

**LSTM**: Long Short-Term Memory; RNN variant for long sequences.

---

## M

**MAE**: Mean Absolute Error.

**MLE**: Maximum Likelihood Estimation.

**MSE**: Mean Squared Error.

**Momentum**: Accelerates gradient descent using past gradients.

---

## N

**Neural Network**: Model composed of connected layers of neurons.

**Normalization**: Scaling features to standard range.

---

## O

**One-Hot Encoding**: Binary vector representation of categorical variables.

**Optimizer**: Algorithm for updating model parameters.

**Overfitting**: Model fits training data too closely, poor generalization.

---

## P

**PCA**: Principal Component Analysis; linear dimensionality reduction.

**Precision**: True positives / (True positives + False positives).

**Pre-training**: Training on large dataset before fine-tuning.

---

## R

**RÂ² Score**: Coefficient of determination; variance explained by model.

**Recall**: True positives / (True positives + False negatives).

**Regression**: Predicting continuous values.

**Regularization**: Techniques to prevent overfitting.

**ReLU**: Rectified Linear Unit; max(0, x).

**RNN**: Recurrent Neural Network; processes sequences.

---

## S

**SGD**: Stochastic Gradient Descent.

**Sigmoid**: Function mapping to (0, 1): 1/(1+e^-x).

**Softmax**: Function converting logits to probabilities.

**SVM**: Support Vector Machine.

---

## T

**Tensor**: Multi-dimensional array.

**Test Set**: Data for final model evaluation.

**Training Set**: Data used to learn model parameters.

**Transfer Learning**: Applying knowledge from one task to another.

**Transformer**: Architecture using self-attention for sequences.

---

## U

**Underfitting**: Model too simple to capture underlying pattern.

---

## V

**Validation Set**: Data for hyperparameter tuning.

**Variance (model)**: Sensitivity of model to training data variations.

**VC Dimension**: Vapnik-Chervonenkis dimension; measure of model capacity.

---

## W

**Weight Decay**: L2 regularization in neural networks.

**Weight Initialization**: Starting values for model parameters.
