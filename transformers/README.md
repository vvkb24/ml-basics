# Transformers: From Theory to Implementation

This module provides a comprehensive, mathematically rigorous guide to Transformer architectures, from self-attention fundamentals to modern Large Language Models.

## ğŸ“š Learning Path

| Order | Topic | Description |
|-------|-------|-------------|
| 1 | [Self-Attention](./self_attention/) | The core attention mechanism |
| 2 | [Positional Encoding](./positional_encoding/) | How transformers understand position |
| 3 | [Transformer Architecture](./transformer_from_scratch/) | Full encoder-decoder implementation |
| 4 | [LLM Fundamentals](./llm_fundamentals/) | Modern large language models |

## ğŸ¯ What You'll Learn

### Mathematical Foundations
- Query, Key, Value intuition and derivation
- Scaled dot-product attention
- Multi-head attention mathematics
- Positional encoding (sinusoidal, RoPE, ALiBi)
- Layer normalization and residual connections

### Practical Implementations
- Self-attention from scratch (NumPy)
- Full Transformer encoder-decoder
- GPT-style decoder-only model
- Multiple positional encoding schemes

### Modern LLM Concepts
- Training pipelines (pretraining, SFT, RLHF)
- Inference optimization (KV cache, quantization)
- Prompting techniques (few-shot, CoT)

## ğŸ“ Module Structure

```
transformers/
â”œâ”€â”€ README.md                      # This file
â”œâ”€â”€ self_attention/
â”‚   â”œâ”€â”€ theory.md                  # Complete attention mathematics
â”‚   â”œâ”€â”€ scratch.py                 # NumPy implementation
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ positional_encoding/
â”‚   â”œâ”€â”€ theory.md                  # Sinusoidal, RoPE, ALiBi theory
â”‚   â”œâ”€â”€ scratch.py                 # All encoding implementations
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ transformer_from_scratch/
â”‚   â”œâ”€â”€ theory.md                  # Full architecture guide
â”‚   â”œâ”€â”€ scratch.py                 # Complete transformer + GPT
â”‚   â””â”€â”€ README.md
â””â”€â”€ llm_fundamentals/
    â”œâ”€â”€ theory.md                  # LLM training, scaling, RLHF
    â”œâ”€â”€ scratch.py                 # Minimal GPT implementation
    â””â”€â”€ README.md
```

## ğŸš€ Quick Start

```python
# Run the self-attention demo
python self_attention/scratch.py

# Run the positional encoding demo
python positional_encoding/scratch.py

# Run the full transformer demo
python transformer_from_scratch/scratch.py

# Run the GPT demo
python llm_fundamentals/scratch.py
```

## ğŸ“– Essential Resources

### Papers
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) (Vaswani et al., 2017)
- [BERT](https://arxiv.org/abs/1810.04805) (Devlin et al., 2018)
- [GPT-3](https://arxiv.org/abs/2005.14165) (Brown et al., 2020)
- [LLaMA](https://arxiv.org/abs/2302.13971) (Touvron et al., 2023)

### Visual Guides
- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)
- [The Annotated Transformer](https://nlp.seas.harvard.edu/annotated-transformer/)
- [Lilian Weng's Attention Guide](https://lilianweng.github.io/posts/2018-06-24-attention/)

### Video Courses
- [Stanford CS224N](https://www.youtube.com/playlist?list=PLoROMvodv4rOSH4v6133s9LFPRHjEmbmJ)
- [Andrej Karpathy: Let's build GPT](https://www.youtube.com/watch?v=kCc8FmEb1nY)
- [3Blue1Brown: Attention](https://www.youtube.com/watch?v=eMlx5fFNoYc)

### Code Repositories
- [nanoGPT](https://github.com/karpathy/nanoGPT) â€” Minimal GPT training
- [HuggingFace Transformers](https://github.com/huggingface/transformers)
- [LLaMA](https://github.com/facebookresearch/llama)

## ğŸ§® Prerequisites

Before diving in, ensure familiarity with:
- Linear algebra (matrix multiplication, eigenvalues)
- Calculus (gradients, chain rule)
- Basic neural networks (MLPs, backpropagation)
- Python and NumPy

See [docs/math_prerequisites/](../docs/math_prerequisites/) for refreshers.
