{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Linear Regression Experiments\n",
                "\n",
                "Interactive exploration of linear regression concepts."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.metrics import mean_squared_error, r2_score\n",
                "\n",
                "# Import our from-scratch implementation\n",
                "import sys\n",
                "sys.path.insert(0, '.')\n",
                "from scratch import LinearRegressionScratch, RidgeRegressionScratch\n",
                "\n",
                "np.random.seed(42)\n",
                "plt.style.use('seaborn-v0_8-whitegrid')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Simple Linear Regression\n",
                "\n",
                "Visualize fitting a line to 2D data."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate 1D data\n",
                "n = 50\n",
                "X = np.random.uniform(0, 10, n).reshape(-1, 1)\n",
                "y = 2 + 3 * X.ravel() + np.random.randn(n) * 2\n",
                "\n",
                "# Fit model\n",
                "model = LinearRegressionScratch(method='normal_equation')\n",
                "model.fit(X, y)\n",
                "\n",
                "# Plot\n",
                "plt.figure(figsize=(10, 6))\n",
                "plt.scatter(X, y, alpha=0.7, label='Data')\n",
                "\n",
                "# Regression line\n",
                "X_line = np.linspace(0, 10, 100).reshape(-1, 1)\n",
                "y_line = model.predict(X_line)\n",
                "plt.plot(X_line, y_line, 'r-', linewidth=2, label=f'ŷ = {model.bias_:.2f} + {model.weights_[0]:.2f}x')\n",
                "\n",
                "plt.xlabel('X')\n",
                "plt.ylabel('y')\n",
                "plt.title('Simple Linear Regression')\n",
                "plt.legend()\n",
                "plt.show()\n",
                "\n",
                "print(f\"True: y = 2 + 3x\")\n",
                "print(f\"Learned: y = {model.bias_:.2f} + {model.weights_[0]:.2f}x\")\n",
                "print(f\"R² = {model.score(X, y):.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Normal Equation vs. Gradient Descent\n",
                "\n",
                "Compare the two optimization methods."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate data\n",
                "n = 100\n",
                "X = np.random.randn(n, 3)\n",
                "true_w = np.array([2, -1, 0.5])\n",
                "y = X @ true_w + 1 + np.random.randn(n) * 0.2\n",
                "\n",
                "# Normal equation\n",
                "model_ne = LinearRegressionScratch(method='normal_equation')\n",
                "model_ne.fit(X, y)\n",
                "\n",
                "# Gradient descent\n",
                "model_gd = LinearRegressionScratch(method='gradient_descent')\n",
                "model_gd.fit(X, y, learning_rate=0.1, n_iterations=500)\n",
                "\n",
                "# Compare\n",
                "print(f\"True weights:     {true_w}\")\n",
                "print(f\"Normal equation:  {model_ne.weights_}\")\n",
                "print(f\"Gradient descent: {model_gd.weights_}\")\n",
                "print(f\"\\nTrue bias: 1.0\")\n",
                "print(f\"Normal equation bias:  {model_ne.bias_:.4f}\")\n",
                "print(f\"Gradient descent bias: {model_gd.bias_:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot gradient descent convergence\n",
                "plt.figure(figsize=(10, 4))\n",
                "plt.plot(model_gd.loss_history_)\n",
                "plt.xlabel('Iteration')\n",
                "plt.ylabel('Loss')\n",
                "plt.title('Gradient Descent Convergence')\n",
                "plt.show()\n",
                "\n",
                "print(f\"Initial loss: {model_gd.loss_history_[0]:.4f}\")\n",
                "print(f\"Final loss: {model_gd.loss_history_[-1]:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Effect of Learning Rate\n",
                "\n",
                "Explore how learning rate affects convergence."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "learning_rates = [0.001, 0.01, 0.1, 0.5]\n",
                "\n",
                "plt.figure(figsize=(12, 4))\n",
                "\n",
                "for lr in learning_rates:\n",
                "    model = LinearRegressionScratch(method='gradient_descent')\n",
                "    model.fit(X, y, learning_rate=lr, n_iterations=200)\n",
                "    plt.plot(model.loss_history_, label=f'lr={lr}')\n",
                "\n",
                "plt.xlabel('Iteration')\n",
                "plt.ylabel('Loss')\n",
                "plt.title('Effect of Learning Rate')\n",
                "plt.legend()\n",
                "plt.ylim(0, 2)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Overfitting and Regularization\n",
                "\n",
                "Demonstrate overfitting with polynomial features and fix with regularization."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate non-linear data\n",
                "n = 20\n",
                "X = np.sort(np.random.uniform(0, 1, n))\n",
                "y = np.sin(2 * np.pi * X) + np.random.randn(n) * 0.3\n",
                "\n",
                "# Create polynomial features\n",
                "def poly_features(x, degree):\n",
                "    return np.column_stack([x**i for i in range(1, degree + 1)])\n",
                "\n",
                "degrees = [1, 3, 9, 15]\n",
                "\n",
                "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
                "\n",
                "X_plot = np.linspace(0, 1, 100)\n",
                "\n",
                "for ax, degree in zip(axes, degrees):\n",
                "    X_poly = poly_features(X, degree)\n",
                "    X_plot_poly = poly_features(X_plot, degree)\n",
                "    \n",
                "    model = LinearRegression()\n",
                "    model.fit(X_poly, y)\n",
                "    y_plot = model.predict(X_plot_poly)\n",
                "    \n",
                "    ax.scatter(X, y, alpha=0.7)\n",
                "    ax.plot(X_plot, y_plot, 'r-')\n",
                "    ax.plot(X_plot, np.sin(2 * np.pi * X_plot), 'g--', alpha=0.5, label='True')\n",
                "    ax.set_title(f'Degree {degree}')\n",
                "    ax.set_ylim(-2, 2)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Ridge regularization on high-degree polynomial\n",
                "degree = 15\n",
                "X_poly = poly_features(X, degree)\n",
                "X_plot_poly = poly_features(X_plot, degree)\n",
                "\n",
                "alphas = [0, 0.001, 0.01, 0.1]\n",
                "\n",
                "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
                "\n",
                "for ax, alpha in zip(axes, alphas):\n",
                "    if alpha == 0:\n",
                "        model = LinearRegression()\n",
                "    else:\n",
                "        model = Ridge(alpha=alpha)\n",
                "    \n",
                "    model.fit(X_poly, y)\n",
                "    y_plot = model.predict(X_plot_poly)\n",
                "    \n",
                "    ax.scatter(X, y, alpha=0.7)\n",
                "    ax.plot(X_plot, y_plot, 'r-')\n",
                "    ax.plot(X_plot, np.sin(2 * np.pi * X_plot), 'g--', alpha=0.5)\n",
                "    ax.set_title(f'Ridge α={alpha}')\n",
                "    ax.set_ylim(-2, 2)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Ridge vs Lasso: Sparsity\n",
                "\n",
                "Compare coefficient shrinkage patterns."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate data with some irrelevant features\n",
                "n = 100\n",
                "X = np.random.randn(n, 10)\n",
                "# Only first 3 features are relevant\n",
                "true_coef = np.array([3, -2, 1, 0, 0, 0, 0, 0, 0, 0])\n",
                "y = X @ true_coef + np.random.randn(n) * 0.5\n",
                "\n",
                "# Compare coefficients\n",
                "linear = LinearRegression().fit(X, y)\n",
                "ridge = Ridge(alpha=1.0).fit(X, y)\n",
                "lasso = Lasso(alpha=0.1).fit(X, y)\n",
                "\n",
                "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
                "\n",
                "x_pos = np.arange(10)\n",
                "\n",
                "axes[0].bar(x_pos, linear.coef_)\n",
                "axes[0].set_title('Linear Regression')\n",
                "axes[0].set_xlabel('Feature')\n",
                "axes[0].set_ylabel('Coefficient')\n",
                "\n",
                "axes[1].bar(x_pos, ridge.coef_)\n",
                "axes[1].set_title('Ridge (α=1.0)')\n",
                "axes[1].set_xlabel('Feature')\n",
                "\n",
                "axes[2].bar(x_pos, lasso.coef_)\n",
                "axes[2].set_title('Lasso (α=0.1)')\n",
                "axes[2].set_xlabel('Feature')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(f\"True coefficients: {true_coef}\")\n",
                "print(f\"Lasso non-zero:    {np.sum(lasso.coef_ != 0)} / 10\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Bias-Variance Tradeoff\n",
                "\n",
                "Visualize the tradeoff with different regularization strengths."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.model_selection import cross_val_score\n",
                "\n",
                "# Generate data\n",
                "n = 100\n",
                "X = np.random.randn(n, 5)\n",
                "y = X @ np.array([1, 2, 3, 4, 5]) + np.random.randn(n) * 2\n",
                "\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
                "\n",
                "alphas = np.logspace(-4, 4, 50)\n",
                "train_scores = []\n",
                "test_scores = []\n",
                "\n",
                "for alpha in alphas:\n",
                "    model = Ridge(alpha=alpha)\n",
                "    model.fit(X_train, y_train)\n",
                "    train_scores.append(model.score(X_train, y_train))\n",
                "    test_scores.append(model.score(X_test, y_test))\n",
                "\n",
                "plt.figure(figsize=(10, 5))\n",
                "plt.semilogx(alphas, train_scores, label='Train R²')\n",
                "plt.semilogx(alphas, test_scores, label='Test R²')\n",
                "plt.xlabel('Regularization (α)')\n",
                "plt.ylabel('R² Score')\n",
                "plt.title('Bias-Variance Tradeoff')\n",
                "plt.legend()\n",
                "plt.axvline(alphas[np.argmax(test_scores)], color='r', linestyle='--', alpha=0.5)\n",
                "plt.show()\n",
                "\n",
                "best_alpha = alphas[np.argmax(test_scores)]\n",
                "print(f\"Best α: {best_alpha:.4f}\")\n",
                "print(f\"Best test R²: {max(test_scores):.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Residual Analysis\n",
                "\n",
                "Diagnose model fit using residual plots."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate well-specified data\n",
                "n = 100\n",
                "X = np.random.randn(n, 1)\n",
                "y = 2 + 3*X.ravel() + np.random.randn(n) * 0.5\n",
                "\n",
                "model = LinearRegression().fit(X, y)\n",
                "y_pred = model.predict(X)\n",
                "residuals = y - y_pred\n",
                "\n",
                "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
                "\n",
                "# Residuals vs Predicted\n",
                "axes[0].scatter(y_pred, residuals, alpha=0.6)\n",
                "axes[0].axhline(0, color='r', linestyle='--')\n",
                "axes[0].set_xlabel('Predicted')\n",
                "axes[0].set_ylabel('Residual')\n",
                "axes[0].set_title('Residuals vs Predicted')\n",
                "\n",
                "# Histogram of residuals\n",
                "axes[1].hist(residuals, bins=20, edgecolor='black')\n",
                "axes[1].set_xlabel('Residual')\n",
                "axes[1].set_ylabel('Frequency')\n",
                "axes[1].set_title('Residual Distribution')\n",
                "\n",
                "# Q-Q plot\n",
                "from scipy import stats\n",
                "stats.probplot(residuals, dist='norm', plot=axes[2])\n",
                "axes[2].set_title('Q-Q Plot')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Comparison with sklearn\n",
                "\n",
                "Verify our implementation matches sklearn."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate data\n",
                "n = 100\n",
                "X = np.random.randn(n, 5)\n",
                "y = X @ np.array([1, 2, 3, 4, 5]) + 2 + np.random.randn(n) * 0.5\n",
                "\n",
                "# Our implementation\n",
                "model_scratch = LinearRegressionScratch(method='normal_equation')\n",
                "model_scratch.fit(X, y)\n",
                "\n",
                "# sklearn\n",
                "model_sklearn = LinearRegression()\n",
                "model_sklearn.fit(X, y)\n",
                "\n",
                "print(\"Weights comparison:\")\n",
                "print(f\"Scratch:  {model_scratch.weights_}\")\n",
                "print(f\"sklearn:  {model_sklearn.coef_}\")\n",
                "print(f\"\\nBias comparison:\")\n",
                "print(f\"Scratch:  {model_scratch.bias_:.6f}\")\n",
                "print(f\"sklearn:  {model_sklearn.intercept_:.6f}\")\n",
                "print(f\"\\nR² comparison:\")\n",
                "print(f\"Scratch:  {model_scratch.score(X, y):.6f}\")\n",
                "print(f\"sklearn:  {model_sklearn.score(X, y):.6f}\")\n",
                "print(f\"\\nMax weight difference: {np.max(np.abs(model_scratch.weights_ - model_sklearn.coef_)):.10f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Real World Example: California Housing\n",
                "\n",
                "Apply linear regression to a real dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.datasets import fetch_california_housing\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "\n",
                "# Load data\n",
                "housing = fetch_california_housing()\n",
                "X, y = housing.data, housing.target\n",
                "feature_names = housing.feature_names\n",
                "\n",
                "print(f\"Dataset shape: {X.shape}\")\n",
                "print(f\"Features: {feature_names}\")\n",
                "\n",
                "# Split and scale\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
                "\n",
                "scaler = StandardScaler()\n",
                "X_train_scaled = scaler.fit_transform(X_train)\n",
                "X_test_scaled = scaler.transform(X_test)\n",
                "\n",
                "# Train models\n",
                "linear = LinearRegression().fit(X_train_scaled, y_train)\n",
                "ridge = Ridge(alpha=1.0).fit(X_train_scaled, y_train)\n",
                "\n",
                "print(f\"\\nLinear Regression - Test R²: {linear.score(X_test_scaled, y_test):.4f}\")\n",
                "print(f\"Ridge Regression - Test R²: {ridge.score(X_test_scaled, y_test):.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Feature importance\n",
                "importance = pd.DataFrame({\n",
                "    'Feature': feature_names,\n",
                "    'Coefficient': linear.coef_,\n",
                "    'Abs Coefficient': np.abs(linear.coef_)\n",
                "}).sort_values('Abs Coefficient', ascending=False)\n",
                "\n",
                "plt.figure(figsize=(10, 5))\n",
                "plt.barh(importance['Feature'], importance['Coefficient'])\n",
                "plt.xlabel('Coefficient')\n",
                "plt.title('Feature Importance (Linear Regression)')\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Key Takeaways\n",
                "\n",
                "1. **Normal equation** gives exact solution but is expensive for high dimensions\n",
                "2. **Gradient descent** scales better but requires tuning learning rate\n",
                "3. **Regularization** prevents overfitting by shrinking weights\n",
                "4. **Lasso** produces sparse solutions (feature selection)\n",
                "5. **Ridge** shrinks all weights but none to zero\n",
                "6. **Residual analysis** helps diagnose model quality"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}